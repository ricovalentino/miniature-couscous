from __future__ import absolute_import

import argparse
import logging

import apache_beam as beam
import pyarrow
from apache_beam.options.pipeline_options import PipelineOptions
from google.cloud import bigquery


def get_parquet_schema(project, dataset, table):
    project = project
    dataset_id = dataset
    table_id = table
    data_type_mapping = {
        'STRING': pyarrow.string(),
        'BYTES': pyarrow.string(),
        'INTEGER': pyarrow.int64(),
        'FLOAT': pyarrow.float64(),
        'BOOLEAN': pyarrow.bool_(),
        'TIMESTAMP': pyarrow.timestamp(unit='s'),
        'DATE': pyarrow.date64(),
        'DATETIME': pyarrow.timestamp(unit='s'),
        # 'ARRAY': pyarrow.list_(),
        # 'RECORD': pyarrow.dictionary()
    }
    client = bigquery.Client(project=project)
    dataset_ref = client.dataset(dataset_id, project=project)
    table_ref = dataset_ref.table(table_id)
    table = client.get_table(table_ref)
    parquet_schema = pyarrow.schema([])
    for f in table.schema:
        parquet_schema = parquet_schema.append(pyarrow.field(f.name, data_type_mapping[f.field_type]))
    return parquet_schema


def run(argv=None):
    """Main entry point: defines and runs the BQ extraction pipeline"""
    parser = argparse.ArgumentParser()
    # custom arguments for bigquery SQL and GCS output location
    parser.add_argument('--bql',
                        dest='bql',
                        help='bigquery sql to extract req columns and rows.')
    parser.add_argument('--output',
                        dest='output',
                        help='gcs output location for parquet files.')
    known_args, pipeline_args = parser.parse_known_args(argv)
    bq_table_source = known_args.bql.split('`')[1].split('.')
    parquet_schema = get_parquet_schema(bq_table_source[0], bq_table_source[1], bq_table_source[2])
    options = PipelineOptions(pipeline_args)

    # instantiate a pipeline with all the pipeline option
    p = beam.Pipeline(options=options)
    # processing and structure of pipeline
    p \
    | 'Input: QueryTable' >> beam.io.Read(beam.io.BigQuerySource(
        query=known_args.bql,
        use_standard_sql=True)) \
    | 'Output: Export to Parquet' >> beam.io.parquetio.WriteToParquet(
        file_path_prefix=known_args.output,
        schema=parquet_schema,
        file_name_suffix='.parquet'
    )

    result = p.run()
    result.wait_until_finish()  # Makes job to display all the logs


if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()
